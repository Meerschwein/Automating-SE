{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UGbZHK27Hzsv"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZDmV62z_0hTl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "from transformers import (\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaForTokenClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "zzAVnKnhJtaM"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "dataset_percent = 1 # 0.1 => 10%\n",
        "\n",
        "train_percent = 0.75\n",
        "eval_percent  = 0.10\n",
        "test_percent  = 0.15\n",
        "\n",
        "# <1 => undersampling\n",
        "# >1 => oversampling\n",
        "# \"vuln\"/\"nonvuln\" => adjust number of samples to this dataset\n",
        "training_sample_nonvuln = 0.5\n",
        "training_sample_vuln    = \"nonvuln\"\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "tokenizer_name        = \"neulab/codebert-cpp\"\n",
        "fn_level_model_name   = \"neulab/codebert-cpp\"\n",
        "line_level_model_name = \"neulab/codebert-cpp\"\n",
        "\n",
        "use_tokenizer_max_length = True # if False: use below\n",
        "tokenizer_max_length     = 2048\n",
        "\n",
        "download_model = True\n",
        "\n",
        "fn_level_trainer_args = TrainingArguments(\n",
        "    output_dir=\"./fn-level\",\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=epochs,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"mcc\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "line_level_trainer_args = TrainingArguments(\n",
        "    output_dir=\"./line-level\",\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=epochs,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"mcc\",\n",
        "    greater_is_better=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PdI6tpGKzOhJ"
      },
      "outputs": [],
      "source": [
        "assert train_percent + eval_percent + test_percent == 1\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "id": "pue27eih1OCL"
      },
      "outputs": [],
      "source": [
        "bugvul_zip_url = \"https://raw.githubusercontent.com/Meerschwein/Automating-SE/refs/heads/main/Big-Vul-dataset.zip\"\n",
        "data_path = \"Big-Vul-dataset/data.json\"\n",
        "\n",
        "if not os.path.exists(\"Big-Vul-dataset.zip\"):\n",
        "    urllib.request.urlretrieve(bugvul_zip_url, \"Big-Vul-dataset.zip\")\n",
        "\n",
        "if not os.path.exists(\"Big-Vul-dataset\"):\n",
        "    with zipfile.ZipFile(\"Big-Vul-dataset.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"Big-Vul-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TfRyATmL5bP0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(data_path, dtype={\"vul\": \"int8\"})\n",
        "\n",
        "df = (df.drop([\"bigvul_id\"], axis=1)\n",
        "        .rename(columns={\"vul\": \"labels\"})\n",
        "        .dropna(subset=[\"code\", \"labels\"])\n",
        "        .drop_duplicates(\"code\")\n",
        "        .reset_index(drop=True))\n",
        "\n",
        "if 0 < dataset_percent < 1: # smaller for training\n",
        "    df, _ = train_test_split(df, test_size=1-dataset_percent, stratify=df['labels'], random_state=seed)\n",
        "\n",
        "train_df, eval_test_df = train_test_split(df, train_size=train_percent, stratify=df['labels'], random_state=seed)\n",
        "eval_df, test_df = train_test_split(eval_test_df, test_size=test_percent/(test_percent+eval_percent), stratify=eval_test_df['labels'], random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_class_distribution(name, df):\n",
        "    class_counts = df[\"labels\"].value_counts().sort_index()\n",
        "    total = len(df)\n",
        "    print(f\"{name} - {total}\")\n",
        "    for label, count in class_counts.items():\n",
        "        percent = (count / total) * 100\n",
        "        l = \"Vuln   \" if label == 1 else \"Nonvuln\"\n",
        "        print(f\"    {l} {count} ({percent:.2f}%)\")\n",
        "\n",
        "def print_all_class_distributions():\n",
        "    print_class_distribution(\"Training\", train_df)\n",
        "    print_class_distribution(\"Validation\", eval_df)\n",
        "    print_class_distribution(\"Test\", test_df)\n",
        "\n",
        "print_all_class_distributions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTEjLe2PZcsW",
        "outputId": "6cc87820-22ca-4462-b630-290cc2adc4fd"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - 139706\n",
            "    Nonvuln 133120 (95.29%)\n",
            "    Vuln    6586 (4.71%)\n",
            "Validation - 18627\n",
            "    Nonvuln 17749 (95.29%)\n",
            "    Vuln    878 (4.71%)\n",
            "Test - 27942\n",
            "    Nonvuln 26624 (95.28%)\n",
            "    Vuln    1318 (4.72%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_vuln    = train_df[train_df['labels'] == 1]\n",
        "train_nonvuln = train_df[train_df['labels'] == 0]\n",
        "\n",
        "def sample_class(df_class, rule):\n",
        "    if isinstance(rule, float) and rule != 1:\n",
        "        if rule < 1:\n",
        "            return df_class.sample(frac=rule, random_state=seed)\n",
        "        else:\n",
        "            return resample(df_class, replace=True, n_samples=int(len(df_class) * rule), random_state=seed)\n",
        "    elif isinstance(rule, int):\n",
        "        if rule == 1:\n",
        "            return df_class\n",
        "        else:\n",
        "            return resample(df_class, replace=True, n_samples=len(df_class) * rule, random_state=seed)\n",
        "    elif rule == \"vuln\":\n",
        "        return resample(df_class, replace=True, n_samples=len(train_vuln), random_state=seed)\n",
        "    elif rule == \"nonvuln\":\n",
        "        return resample(df_class, replace=True, n_samples=len(train_nonvuln), random_state=seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid rule: {rule}\")\n",
        "\n",
        "train_nonvuln = sample_class(train_nonvuln, training_sample_nonvuln)\n",
        "train_vuln    = sample_class(train_vuln, training_sample_vuln)\n",
        "\n",
        "train_df = pd.concat([train_vuln, train_nonvuln]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "print_all_class_distributions()\n",
        "\n",
        "raw_train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "raw_eval_ds  = Dataset.from_pandas(eval_df, preserve_index=False)\n",
        "raw_test_ds  = Dataset.from_pandas(test_df, preserve_index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUQ2AZ8XMRD",
        "outputId": "598130fc-ecbc-4780-a329-96144953bc5d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - 133120\n",
            "    Nonvuln 66560 (50.00%)\n",
            "    Vuln    66560 (50.00%)\n",
            "Validation - 18627\n",
            "    Nonvuln 17749 (95.29%)\n",
            "    Vuln    878 (4.71%)\n",
            "Test - 27942\n",
            "    Nonvuln 26624 (95.28%)\n",
            "    Vuln    1318 (4.72%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_name)"
      ],
      "metadata": {
        "id": "Xrkq1q8Bd3xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4SL6CGbTIA7W"
      },
      "outputs": [],
      "source": [
        "def tokenize(batch):\n",
        "    max_length = tokenizer.model_max_length if use_tokenizer_max_length else tokenizer_max_length\n",
        "    return tokenizer(batch[\"code\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "fn_level_train_ds = raw_train_ds.map(tokenize, batched=True, remove_columns=[\"code\"])\n",
        "fn_level_eval_ds  = raw_eval_ds.map(tokenize, batched=True, remove_columns=[\"code\"])\n",
        "fn_level_test_ds  = raw_test_ds.map(tokenize, batched=True, remove_columns=[\"code\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kQoUpyPhILFX"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "mcc_metric = evaluate.load(\"matthews_correlation\")\n",
        "auc_metric = evaluate.load(\"roc_auc\")\n",
        "\n",
        "metrics_include_report = False\n",
        "\n",
        "def fn_level_compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()  # Probability of class 1 (vulnerable)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    mcc = mcc_metric.compute(predictions=predictions, references=labels)[\"matthews_correlation\"]\n",
        "    auc = auc_metric.compute(prediction_scores=probs, references=labels)[\"roc_auc\"]\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "\n",
        "    if metrics_include_report:\n",
        "        report = classification_report(labels, predictions, target_names=[\"Non-vulnerable\", \"Vulnerable\"])\n",
        "        metrics[\"report\"] = report\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def test_model(trainer, test_dataset):\n",
        "    global metrics_include_report\n",
        "    metrics_include_report = True\n",
        "    evaluation_results = trainer.evaluate(test_dataset)\n",
        "    evaluation_df = pd.DataFrame([evaluation_results])\n",
        "    evaluation_df.columns = evaluation_df.columns.str.replace('eval_', '')\n",
        "    evaluation_df = evaluation_df.drop([\"samples_per_second\", \"steps_per_second\", \"epoch\", \"runtime\", \"report\", \"loss\"], axis=1)\n",
        "    display(evaluation_df)\n",
        "    print(evaluation_results[\"eval_report\"])\n",
        "    metrics_include_report = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eOK1vNgU6gIr"
      },
      "outputs": [],
      "source": [
        "fn_level_model = RobertaForSequenceClassification.from_pretrained(fn_level_model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eN-vEYfMrDK"
      },
      "outputs": [],
      "source": [
        "fn_level_trainer = Trainer(\n",
        "    args=fn_level_trainer_args,\n",
        "    model=fn_level_model,\n",
        "    train_dataset=fn_level_train_ds,\n",
        "    eval_dataset=fn_level_eval_ds,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=fn_level_compute_metrics,\n",
        ")\n",
        "\n",
        "fn_level_trainer.train()\n",
        "fn_level_trainer.save_model(\"fn-level-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb_ocDNSiB4A"
      },
      "outputs": [],
      "source": [
        "test_model(fn_level_trainer, fn_level_test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_model(dir):\n",
        "    files.download(shutil.make_archive(dir, 'zip', dir))"
      ],
      "metadata": {
        "id": "Sx0E-NT3Ay8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jl2-TgflGqA4"
      },
      "outputs": [],
      "source": [
        "if download_model:\n",
        "    download_model(\"./fn-level-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PARle_GS5Vd"
      },
      "outputs": [],
      "source": [
        "def add_token_labels(example):\n",
        "    code        = example[\"code\"]\n",
        "    vuln_lines  = set(example[\"flaw_line_no\"]) # [] if benign\n",
        "\n",
        "    max_length = tokenizer.model_max_length if use_tokenizer_max_length else tokenizer_max_length\n",
        "    enc = tokenizer(\n",
        "        code,\n",
        "        return_offsets_mapping=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # map every token to its source-code line\n",
        "    labels = np.full(len(enc[\"input_ids\"]), -100, dtype=np.int8) # pad value\n",
        "    line_start = [0] + [i + 1 for i, c in enumerate(code) if c == \"\\n\"]\n",
        "\n",
        "    for idx, (start, _) in enumerate(enc[\"offset_mapping\"]):\n",
        "        if start == 0 and idx == 0: # [CLS] token => keep -100\n",
        "            continue\n",
        "        # line numbers are 1-based\n",
        "        line_no = 1 + sum(start >= ls for ls in line_start)\n",
        "        labels[idx] = int(line_no in vuln_lines)\n",
        "\n",
        "    enc.pop(\"offset_mapping\")\n",
        "    enc[\"labels\"] = labels.tolist()\n",
        "    return enc\n",
        "\n",
        "line_level_train_ds = raw_train_ds.map(add_token_labels, remove_columns=list(train_df.columns))\n",
        "line_level_eval_ds  = raw_eval_ds.map(add_token_labels, remove_columns=list(train_df.columns))\n",
        "line_level_test_ds  = raw_test_ds.map(add_token_labels, remove_columns=list(train_df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-bmlLOVUpuk"
      },
      "outputs": [],
      "source": [
        "metrics_include_report = False\n",
        "def line_level_metrics(eval_pred):\n",
        "    logits, y = eval_pred\n",
        "    logits_flat = logits.reshape(-1, logits.shape[-1]) # Flatten logits for masking\n",
        "    p = logits.argmax(-1).flatten()\n",
        "    y = y.flatten()\n",
        "    mask = y != -100 # ignore padding tokens\n",
        "    predictions, labels = p[mask], y[mask]\n",
        "    logits_masked = logits_flat[mask]\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(logits_masked), dim=1)[:, 1].numpy()  # Probability of class 1 (vulnerable)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    mcc = mcc_metric.compute(predictions=predictions, references=labels)[\"matthews_correlation\"]\n",
        "    auc = auc_metric.compute(prediction_scores=probs, references=labels)[\"roc_auc\"]\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"mcc\": mcc,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "\n",
        "    if metrics_include_report:\n",
        "        report = classification_report(labels, predictions, target_names=[\"Non-vulnerable\", \"Vulnerable\"])\n",
        "        metrics[\"report\"] = report\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line_level_model = RobertaForTokenClassification.from_pretrained(line_level_model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "wXDl_Z8m9atu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_B26W8qVViF"
      },
      "outputs": [],
      "source": [
        "line_level_trainer = Trainer(\n",
        "    args=line_level_trainer_args,\n",
        "    model=line_level_model,\n",
        "    train_dataset=line_level_train_ds,\n",
        "    eval_dataset=line_level_eval_ds,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=line_level_metrics,\n",
        ")\n",
        "\n",
        "line_level_trainer.train()\n",
        "line_level_trainer.save_model(\"line-level-model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-X67vGJWuZF"
      },
      "outputs": [],
      "source": [
        "test_model(line_level_trainer, line_level_test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0fMDVZSWfsM"
      },
      "outputs": [],
      "source": [
        "if download_model:\n",
        "    download_model(\"line-level-model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith(\".zip\"):\n",
        "        folder_name = filename.replace(\".zip\", \"\")\n",
        "        os.makedirs(folder_name, exist_ok=True)\n",
        "        !unzip -q \"$filename\" -d \"$folder_name\""
      ],
      "metadata": {
        "id": "eF5l8zsmFp-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFUCs2eMGrpP"
      },
      "outputs": [],
      "source": [
        "trained_fn_level_model = RobertaForSequenceClassification.from_pretrained(\"./fn-level-model\")\n",
        "trained_line_level_model = RobertaForTokenClassification.from_pretrained(\"./line-level-model\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "_ = trained_fn_level_model.to(device).eval()\n",
        "_ = trained_line_level_model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWohD27Cqf6e"
      },
      "outputs": [],
      "source": [
        "def get_vuln_lines(example):\n",
        "    code = example[\"code\"]\n",
        "\n",
        "    # Function-level classification\n",
        "    fn_inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
        "    fn_inputs = {k: v.to(device) for k, v in fn_inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        fn_outputs = trained_fn_level_model(**fn_inputs)\n",
        "        fn_probs = torch.softmax(fn_outputs.logits, dim=1)\n",
        "    is_vulnerable = fn_probs[0][1].item() > 0.5  # Class 1 = vulnerable\n",
        "\n",
        "    if not is_vulnerable:\n",
        "        return {\"vulnerable\": False, \"lines\": []}\n",
        "\n",
        "    # Line-level classification\n",
        "    enc = tokenizer(code, return_offsets_mapping=True, return_tensors=\"pt\",\n",
        "                    truncation=True, padding=\"max_length\", max_length=512)\n",
        "    offset_mapping = enc.pop(\"offset_mapping\")[0]\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        line_outputs = trained_line_level_model(**enc)\n",
        "    line_logits = line_outputs.logits\n",
        "    line_preds = torch.argmax(line_logits, dim=-1)[0]  # shape: [seq_len]\n",
        "\n",
        "    # Map tokens to line numbers\n",
        "    lines = code.split('\\n')\n",
        "    line_start_positions = [0]\n",
        "    for line in lines:\n",
        "        line_start_positions.append(line_start_positions[-1] + len(line) + 1)\n",
        "\n",
        "    line_indices = set()\n",
        "    for idx, (start_offset, _) in enumerate(offset_mapping):\n",
        "        if start_offset == 0 and idx == 0:  # [CLS] token\n",
        "            continue\n",
        "        if line_preds[idx].item() == 1:\n",
        "            start = start_offset.item()\n",
        "            line_no = 1 + sum(start >= pos for pos in line_start_positions)\n",
        "            line_indices.add(line_no)\n",
        "\n",
        "    return {\"vulnerable\": True, \"lines\": sorted(line_indices)}\n",
        "\n",
        "def display_vulnerability_result(example, predicted_lines):\n",
        "    code_lines = example[\"code\"].split(\"\\n\")\n",
        "    actual_lines = set(example.get(\"flaw_line_no\", []))\n",
        "    predicted_lines = set(predicted_lines)\n",
        "\n",
        "    max_line_no_width = len(str(len(code_lines)))\n",
        "\n",
        "    print(f\"lines{sorted(actual_lines)} pred{sorted(predicted_lines)}\")\n",
        "    for i, line in enumerate(code_lines, start=1):\n",
        "        line_no = str(i).rjust(max_line_no_width)\n",
        "        actual_flag = \"v\" if i in actual_lines else \" \"\n",
        "        predicted_flag = \"p\" if i in predicted_lines else \" \"\n",
        "        print(f\"{line_no} {actual_flag}{predicted_flag}|{line}\")\n",
        "\n",
        "small_vuln_examples = df[\n",
        "    (df[\"labels\"] == 1) &\n",
        "    (df[\"code\"].apply(lambda c: len(c.splitlines()) <= 10))  # max 7 lines\n",
        "]\n",
        "examples_to_test = small_vuln_examples.sample(n=5, random_state=seed).to_dict(orient=\"records\")\n",
        "\n",
        "for ex in examples_to_test:\n",
        "    result = get_vuln_lines(ex)\n",
        "    print(f\"vuln {ex['labels']==1} pred {result['vulnerable']}\")\n",
        "    display_vulnerability_result(ex, result[\"lines\"])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "keep colab running\n",
        "```js\n",
        "function ClickConnect() {\n",
        "    console.log(\"Working\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "var clicker = setInterval(ClickConnect, 60000);\n",
        "```"
      ],
      "metadata": {
        "id": "xUaOPEz7DYk-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}