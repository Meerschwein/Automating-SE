{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUDUSTcWyi_o"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "T3JCgtavyD6T"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import urllib.request\n",
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizerFast\n",
        "import numpy as np\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mJ1ky1ViyBBm"
      },
      "outputs": [],
      "source": [
        "bugvul_zip_url = \"https://raw.githubusercontent.com/Meerschwein/Automating-SE/refs/heads/main/Big-Vul-dataset.zip\"\n",
        "data_path = \"Big-Vul-dataset/data.json\"\n",
        "\n",
        "if not os.path.exists(\"Big-Vul-dataset.zip\"):\n",
        "    urllib.request.urlretrieve(bugvul_zip_url, \"Big-Vul-dataset.zip\")\n",
        "\n",
        "if not os.path.exists(\"Big-Vul-dataset\"):\n",
        "    with zipfile.ZipFile(\"Big-Vul-dataset.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"Big-Vul-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CemL0DJ8yNK6"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"json\", data_files={\"train\": data_path}, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "g2od2OMVyVfb"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"neulab/codebert-cpp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAXEmQW8y1DT"
      },
      "outputs": [],
      "source": [
        "def count_tokens(batch):\n",
        "    tokenized = tokenizer(batch[\"code\"], truncation=False, padding=False)\n",
        "    return {\"token_count\": [len(ids) for ids in tokenized[\"input_ids\"]]}\n",
        "\n",
        "ds = ds.map(count_tokens, batched=True)\n",
        "vuln_ds = ds.filter(lambda x: x[\"vul\"] == 1)\n",
        "non_vuln_ds = ds.filter(lambda x: x[\"vul\"] == 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv7gxL1s1fMz",
        "outputId": "0fc3a1c1-2a48-405e-91d0-166c66c4b47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== All ===\n",
            "Total examples: 186530\n",
            "Mean tokens: 394.9\n",
            "Median tokens: 167.0\n",
            "Max tokens: 143833\n",
            "\n",
            "Cumulative Token Coverage:\n",
            "≤  512 tokens: 153318 examples (82.2%)\n",
            "≤ 1024 tokens: 172939 examples (92.7%)\n",
            "≤ 2048 tokens: 181935 examples (97.5%)\n",
            "≤ 4096 tokens: 185064 examples (99.2%)\n",
            "≤ 8192 tokens: 186081 examples (99.8%)\n",
            "\n",
            "=== Vulnerable ===\n",
            "Total examples: 8794\n",
            "Mean tokens: 1094.8\n",
            "Median tokens: 418.0\n",
            "Max tokens: 55807\n",
            "\n",
            "Cumulative Token Coverage:\n",
            "≤  512 tokens:   5036 examples (57.3%)\n",
            "≤ 1024 tokens:   6729 examples (76.5%)\n",
            "≤ 2048 tokens:   7802 examples (88.7%)\n",
            "≤ 4096 tokens:   8357 examples (95.0%)\n",
            "≤ 8192 tokens:   8619 examples (98.0%)\n",
            "\n",
            "=== Non-Vulnerable ===\n",
            "Total examples: 177736\n",
            "Mean tokens: 360.3\n",
            "Median tokens: 161.0\n",
            "Max tokens: 143833\n",
            "\n",
            "Cumulative Token Coverage:\n",
            "≤  512 tokens: 148282 examples (83.4%)\n",
            "≤ 1024 tokens: 166210 examples (93.5%)\n",
            "≤ 2048 tokens: 174133 examples (98.0%)\n",
            "≤ 4096 tokens: 176707 examples (99.4%)\n",
            "≤ 8192 tokens: 177462 examples (99.8%)\n"
          ]
        }
      ],
      "source": [
        "def analyze(name, dataset):\n",
        "    counts = np.array(dataset[\"token_count\"])\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"Total examples: {len(dataset)}\")\n",
        "\n",
        "    print(f\"Mean tokens: {counts.mean():.1f}\")\n",
        "    print(f\"Median tokens: {np.median(counts):.1f}\")\n",
        "    print(f\"Max tokens: {counts.max()}\")\n",
        "\n",
        "    thresholds = [512, 1024, 2048, 4096, 8192]\n",
        "    print(\"\\nCumulative Token Coverage:\")\n",
        "    for threshold in thresholds:\n",
        "        num_under = (counts <= threshold).sum()\n",
        "        percent_under = (num_under / len(counts)) * 100\n",
        "        print(f\"<= {threshold:4} tokens: {num_under:6} examples ({percent_under:.1f}%)\")\n",
        "\n",
        "analyze(\"All\", ds)\n",
        "analyze(\"Vulnerable\", vuln_ds)\n",
        "analyze(\"Non-Vulnerable\", non_vuln_ds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
